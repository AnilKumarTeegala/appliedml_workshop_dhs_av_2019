{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing and Analysis\n",
    "\n",
    "Data Processing is the most important and most time consuming component of the overall lifecycle of any Machine Learning project. \n",
    "\n",
    "In this notebook, we will analyze a dummy dataset to understand different issues we face with real world datasets and steps to handle the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.113Z"
    }
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from utils import generate_sample_data\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset\n",
    "\n",
    "+ Question: Generate 1000 sample rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.205Z"
    }
   },
   "outputs": [],
   "source": [
    "## Generate a dataset with 1000 rows\n",
    "df = generate_sample_data(row_count=1000)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze generated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.213Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe Stats\n",
    "\n",
    "Determine the following:\n",
    "\n",
    "* The number of data points (rows). (*Hint:* check out the dataframe `.shape` attribute.)\n",
    "* The column names. (*Hint:* check out the dataframe `.columns` attribute.)\n",
    "* The data types for each column. (*Hint:* check out the dataframe `.dtypes` attribute.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.220Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of rows::\",df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "+ Get the number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.226Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of columns::\",df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.228Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Column Names::\",df.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.230Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Column Data Types::\\n\",df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.231Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Columns with Missing Values::\",df.columns[df.isnull().any()].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.233Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of rows with Missing Values::\",len(pd.isnull(df).any(1).nonzero()[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.239Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.241Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize Columns\n",
    "\n",
    "### Question\n",
    "+ Use ```columns``` attribute and ```tolist()``` method to get the list of all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.250Z"
    }
   },
   "outputs": [],
   "source": [
    "# list all columns\n",
    "print(\"Dataframe columns:\\n{}\".format(df.columns.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility to Standardize Columns\n",
    "\n",
    "+ Question : We usually use lowercase-snakecased column names in python. Write a utility method to do the same. You may user methods like ```lower, replace```. Setting ```inplace``` = ```True``` avoid creating a copy of your dataframe\n",
    "\n",
    "\n",
    "*Hint:* there are multiple ways to do this, but you could use either the [string processing methods](http://pandas.pydata.org/pandas-docs/stable/text.html) or the [apply method](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.apply.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.258Z"
    }
   },
   "outputs": [],
   "source": [
    "def cleanup_column_names(df,rename_dict={},do_inplace=True):\n",
    "    \"\"\"This function renames columns of a pandas dataframe\n",
    "       It converts column names to snake case if rename_dict is not passed. \n",
    "    Args:\n",
    "        rename_dict (dict): keys represent old column names and values point to \n",
    "                            newer ones\n",
    "        do_inplace (bool): flag to update existing dataframe or return a new one\n",
    "    Returns:\n",
    "        pandas dataframe if do_inplace is set to False, None otherwise\n",
    "\n",
    "    \"\"\"\n",
    "    if not rename_dict:\n",
    "        # lower case and replace <space> with <underscore>\n",
    "        return df.rename(columns={col: col.lower().replace(' ','_') \n",
    "                            for col in df.columns.values.tolist()}, \n",
    "                         inplace=True)\n",
    "    else:\n",
    "        return df.rename(columns=rename_dict,inplace=do_inplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.260Z"
    }
   },
   "outputs": [],
   "source": [
    "cleanup_column_names(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.262Z"
    }
   },
   "outputs": [],
   "source": [
    "# Updated column names\n",
    "print(\"Dataframe columns:\\n{}\".format(df.columns.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort basis specific attributes\n",
    "\n",
    "+ Question: Sort serial_no in ascending and price in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.277Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ascending for Serial No and Descending for Price\n",
    "display(df.sort_values(['serial_no', 'price'], \n",
    "                         ascending=[True, False]).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T18:40:05.505061Z",
     "start_time": "2019-10-26T18:40:05.502620Z"
    }
   },
   "source": [
    "### Reorder columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.287Z"
    }
   },
   "outputs": [],
   "source": [
    "display(df[['serial_no','date','user_id','user_type',\n",
    "              'product_id','quantity_purchased','price']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.295Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using Column Index\n",
    "# print 10 values from column at index 3\n",
    "print(df.iloc[:,3].values[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.297Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using Column Name\n",
    "# print 10 values of quantity purchased\n",
    "print(df.quantity_purchased.values[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.299Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using Datatype\n",
    "# print 10 values of columns with data type float\n",
    "print(df.select_dtypes(include=['float64']).values[:10,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T18:41:50.825231Z",
     "start_time": "2019-10-26T18:41:50.822229Z"
    }
   },
   "source": [
    "### Select Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.308Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using Row Index\n",
    "display(df.iloc[[10,501,20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.310Z"
    }
   },
   "outputs": [],
   "source": [
    "# Exclude specific rows\n",
    "display(df.drop([0,24,51], axis=0).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "+ Show only rows which have quantity purchased greater than 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.318Z"
    }
   },
   "outputs": [],
   "source": [
    "# Conditional Filtering\n",
    "# Quantity_Purchased greater than 25\n",
    "display(df[df.quantity_purchased > 25].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.321Z"
    }
   },
   "outputs": [],
   "source": [
    "# Offset from Top\n",
    "display(df[100:].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.325Z"
    }
   },
   "outputs": [],
   "source": [
    "# Offset from Bottom\n",
    "display(df[-10:].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.335Z"
    }
   },
   "outputs": [],
   "source": [
    "# Existing Datatypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.337Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set Datatime as dtype for date column\n",
    "df['date'] = pd.to_datetime(df.date)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map/Apply Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "+ Write a utility method to create a new column ```user_class``` from ```user_type``` using the following mapping:\n",
    "    - ```user_type``` __a__ and __b__ map to ```user_class``` __new__\n",
    "    - ```user_type``` __c__ maps to ```user_class``` __existing__\n",
    "    - ```user_type``` __d__ maps to ```user_class``` __loyal_existing__\n",
    "    - map all other ```user_type``` values as __error__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.352Z"
    }
   },
   "outputs": [],
   "source": [
    "def expand_user_type(u_type):\n",
    "    \"\"\"This function maps user types to user classes\n",
    "    Args:\n",
    "        u_type (str): user type value\n",
    "    Returns:\n",
    "        (str) user_class value\n",
    "\n",
    "    \"\"\"\n",
    "    if u_type in ['a','b']:\n",
    "        return 'new'\n",
    "    elif u_type == 'c':\n",
    "        return 'existing'\n",
    "    elif u_type == 'd':\n",
    "        return 'loyal_existing'\n",
    "    else:\n",
    "        return 'error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.355Z"
    }
   },
   "outputs": [],
   "source": [
    "# Map User Type to User Class\n",
    "df['user_class'] = df['user_type'].map(expand_user_type)\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "+ Get range for each numeric attribute, i.e. max-min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.363Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply: Using apply to get attribute ranges\n",
    "display(df.select_dtypes(include=[np.number]).apply(lambda x: \n",
    "                                                        x.max()- x.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.365Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply-Map: Extract Week from Date\n",
    "df['purchase_week'] = df[['date']].applymap(lambda dt:dt.week \n",
    "                                                if not pd.isnull(dt.week) \n",
    "                                                else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.368Z"
    }
   },
   "outputs": [],
   "source": [
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.377Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop Rows with Missing Dates\n",
    "df_dropped = df.dropna(subset=['date'])\n",
    "display(df_dropped.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.380Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filling missing price with mean price\n",
    "df_dropped['price'].fillna(value=np.round(df.price.mean(),decimals=2),\n",
    "                                inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.382Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fill missing user types using values from previous row\n",
    "df_dropped['user_type'].fillna(method='ffill',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Duplicates\n",
    "\n",
    "### Question\n",
    "+ Identify duplicates only for column ```serial_no```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.390Z"
    }
   },
   "outputs": [],
   "source": [
    "# sample duplicates. Identify for serial_no\n",
    "display(df_dropped[df_dropped.duplicated(subset=['serial_no'])].head())\n",
    "print(\"Shape of df={}\".format(df_dropped.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.393Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop Duplicates\n",
    "df_dropped.drop_duplicates(subset=['serial_no'],inplace=True)\n",
    "display(df_dropped.head())\n",
    "print(\"Shape of df={}\".format(df_dropped.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "+ Remove rows which have less than 3 attributes with non-missing data\n",
    "+ Print the shape of dataframe thus prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.403Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove rows which have less than 3 attributes with non-missing data\n",
    "display(df.dropna(thresh=3).head())\n",
    "print(\"Shape of df={}\".format(df.dropna(thresh=3).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Categoricals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.421Z"
    }
   },
   "outputs": [],
   "source": [
    "display(pd.get_dummies(df,columns=['user_type']).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "\n",
    "### Question\n",
    "+ Use a dictionary to encode user_types in sequence of numbers. Replace missing/Nan's with -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.430Z"
    }
   },
   "outputs": [],
   "source": [
    "type_map = {'a': 0, 'b': 1, 'c': 2, 'd': 3, np.NAN: -1}\n",
    "df['encoded_user_type'] = df.user_type.map(type_map)\n",
    "display((df.tail()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Numerical Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-Max Scalar\n",
    "### Question\n",
    "+ Control the range of numerical attribute price by using ```MinMaxScaler``` transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.444Z"
    }
   },
   "outputs": [],
   "source": [
    "df_normalized = df.dropna().copy()\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(df_normalized['price'].values.reshape(-1,1))\n",
    "df_normalized['price'] = np_scaled.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.446Z"
    }
   },
   "outputs": [],
   "source": [
    "display(df_normalized.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.454Z"
    }
   },
   "outputs": [],
   "source": [
    "df_normalized = df.dropna().copy()\n",
    "robust_scaler = preprocessing.RobustScaler()\n",
    "rs_scaled = robust_scaler.fit_transform(df_normalized['quantity_purchased'].values.reshape(-1,1))\n",
    "df_normalized['quantity_purchased'] = rs_scaled.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.457Z"
    }
   },
   "outputs": [],
   "source": [
    "display(df_normalized.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group-By\n",
    "\n",
    "### Question\n",
    "+ Group By  attribute ```user_class``` and get sum of quantity_purchased\n",
    "\n",
    "*Hint:* you may want to use Pandas [`groupby` method](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html) to group by certain attributes before calculating the statistic.\n",
    "\n",
    "Try calculating multiple statistics (mean, median, etc) in a single table (i.e. with a single groupby call). See the section of the Pandas documentation on [applying multiple functions at once](http://pandas.pydata.org/pandas-docs/stable/groupby.html#applying-multiple-functions-at-once) for a hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.465Z"
    }
   },
   "outputs": [],
   "source": [
    "# Group By attributes user_class and get sum of quantity_purchased\n",
    "print(df.groupby(['user_class'])['quantity_purchased'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.468Z"
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate Functions. Sum, Mean and Non Zero Row Count\n",
    "display(\n",
    "    df.groupby(['user_class'])['quantity_purchased'].agg(\n",
    "        [np.sum, np.mean, np.count_nonzero]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.470Z"
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate Functions specific to columns\n",
    "display(df.groupby(['user_class','user_type']).agg({'price':np.mean,\n",
    "                                                        'quantity_purchased':np.max}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.473Z"
    }
   },
   "outputs": [],
   "source": [
    "# Multiple Aggregate Functions\n",
    "display(\n",
    "    df.groupby(['user_class', 'user_type']).agg({\n",
    "        'price': {\n",
    "            'total_price': np.sum,\n",
    "            'mean_price': np.mean,\n",
    "            'variance_price': np.std,\n",
    "            'count': np.count_nonzero\n",
    "        },\n",
    "        'quantity_purchased': np.sum\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.481Z"
    }
   },
   "outputs": [],
   "source": [
    "display(df.pivot_table(index='date', columns='user_type', \n",
    "                         values='price',aggfunc=np.mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T18:56:02.626820Z",
     "start_time": "2019-10-26T18:56:02.623564Z"
    }
   },
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:02:30.489Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df.stack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
